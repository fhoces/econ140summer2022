---
title: "Ec140 - Regression as Conditional Expectation "
subtitle: " "
author: "Fernando Hoces la Guardia"
date: "07/15/2022"
output: 
  xaringan::moon_reader:
    footer: "These slides available at https://fhoces.github.io/econ140summer2022/"
    css: [default, metropolis, metropolis-fonts, "my-css.css"] 
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

<style type="text/css">
.remark-slide-content {
    font-size: 30px;
    padding: 1em 1em 1em 1em;
}
</style>

```{r , include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(ggthemes, viridis, knitr, extrafont, tidyverse, magrittr, latex2exp, 
       parallel, Ecdat, wooldridge, dslabs, ggforce, emo, png, grid, pander, 
       countdown, emoGG, haven, broom, ggridges, gridExtra, kableExtra, snakecase, 
       janitor, data.table, lubridate, lfe, here)

# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#3b3b9a"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
slate <- "#314f4f"
# Dark slate grey: #314f4f

options(htmltools.dir.version = FALSE)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])

overwrite = FALSE


# Define colors
red_pink <- "#e64173"
met_slate <- "#23373b" # metropolis font color
# Notes directory
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  #dpi = 300,
  #cache = T,
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE
  #fig.width = 11,
  #fig.height = 5
)

theme_simple <- theme_bw() + theme(
  axis.line = element_line(color = met_slate),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  text = element_text(family = "Fira Sans", color = met_slate, size = 14),
  axis.text.x = element_text(size = 12),
  axis.text.y = element_text(size = 12),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  legend.position = "none"
)
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 18, family = "STIXGeneral"),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_math <- theme_void() + theme(
  text = element_text(family = "MathJax_Math"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_serif <- theme_void() + theme(
  text = element_text(family = "MathJax_Main"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_void() + theme(
  text = element_text(family = "Fira Sans Book"),
  axis.title = element_text(size = 18),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = grey_light,
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_set(theme_gray(base_size = 20))
# Column names for regression results
reg_columns <- c("Term", "Est.", "S.E.", "t stat.", "p-Value")
# Function for formatting p values
format_pvi <- function(pv) {
  return(ifelse(
    pv < 0.0001,
    "<0.0001",
    round(pv, 4) %>% format(scientific = F)
  ))
}
format_pv <- function(pvs) lapply(X = pvs, FUN = format_pvi) %>% unlist()
# Tidy regression results table
tidy_table <- function(x, terms, highlight_row = 1, highlight_color = "black", highlight_bold = T, digits = c(NA, 3, 3, 2, 5), title = NULL) {
  x %>%
    tidy() %>%
    select(1:5) %>%
    mutate(
      term = terms,
      p.value = p.value %>% format_pv()
    ) %>%
    kable(
      col.names = reg_columns,
      escape = F,
      digits = digits,
      caption = title
    ) %>%
    kable_styling(font_size = 20) %>%
    row_spec(1:nrow(tidy(x)), background = "white") %>%
    row_spec(highlight_row, bold = highlight_bold, color = highlight_color)
}


```



```{css, echo = F, eval = T}
@media print {
  .has-continuation {
    display: block !important;
  }
}
```

# Regression Journey

- Regression as Matching on Groups. Ch2 of MM up to page 68 (not included).

- Regression as Line Fitting and Conditional Expectation. Ch2 of MM, Appendix. (Part II today)

- Multiple Regression and Omitted Variable Bias. Ch2 of MM pages 68-79. 

- Regression Inference, Binary Variables and Logarithms. Ch2 of MM, Appendix + others. 

---
# Regression Journey

- Regression as Matching on Groups. Ch2 of MM up to page 68 (not included).

- **Regression as Line Fitting and Conditional Expectation. Ch2 of MM, Appendix. (Part II today)** 

- Multiple Regression and Omitted Variable Bias. Ch2 of MM pages 68-79. 

- Regression Inference, Binary Variables and Logarithms. Ch2 of MM, Appendix + others. 


---

# Regression as Conditional Expectation: Today's Goal

- Similar to line fitting, this topic will be pretty abstract. 

- In a similar spirit, this material is useful but not essential.  

- Put limited effort into understanding it (~today’s class +1-4hrs looking at your notes and reading the appendix of Ch2) but if it doesn’t sink in, don’t get discouraged and move forward into the next topic. 

- For Regression as Conditional Expectations our goals: 
 1. Connect the concept of regression with conditional expectations. 
 1. Show how regression is a simple difference in means within groups defined by regesssors.  
 
- Last mainly theoretical topic of the course!
 

---

# Regression as Conditional Expectation: Outline

- Conditional Expectation and the Conditional Expectation Function (CEF).
  - Definitions
  - Example
  - Why should we care about it (regardless of regression)?
- Connection between CEF and regression.
- Use CEF to solidify our understanding of how regression **is** a simple difference in groups, for groups defined by values of regressors.  
  

---
count:false

# Regression as Conditional Expectation: Outline

- Conditional Expectation and the Conditional Expectation Function (CEF).
  - **Definitions**
  - Example
  - Why should we care about it (regardless of regression)?
- Connection between CEF and regression.
- Use CEF to solidify our understanding of how regression **is** a simple difference in groups, for groups defined by values of regressors.  
  

---

# .font90[Conditional Expectation and Conditional Expectation Function]

- Earlier in the semester we defined the a Conditional Expectation as the population average of one variable, given some value taken by another variable. 

$$
\begin{equation}
\mathop{\mathbb{E}}(Y_i|X_i=x)  = \sum_{y}yP(Y_i=y|X_i=x)
\end{equation}
$$
  
- This average does two things:
  1. it summarizes the distribution (the data) of $Y_i$, given $x$. 
  1. it provides information how $Y_i$ changes as we move $x$. It gave us our first way of thinking about associations (before RCTs and regressions!). As a thought exercise, let's think about the distribution of happiness $(Y_i)$ and the result of some sports outcome $(X_i)$ (or other leisurely activity). 


---
# CEF: Definition

- When we look at a CE, we naturally tend to focus on $Y_i$. But if our goal is to describe $Y_i$, the relevant section to vary in the CE its not $Y_i$, its $X_i$. 

- For this reason when we want to use the concept CE to describe the data $(Y_i)$, with one number (the average), across a range of values of $x$, we define the .hi-pink[*Conditional Expectation Function (CEF).*] as the collection of all such CEs (averages). 

.hi-slate[Examples:]

.pull-left[
- $\mathop{E}\left[ \text{Income}_i \mid \text{Education}_i \right]$
- $\mathop{E}\left[ \text{Birth weight}_i \mid \text{Air quality}_i \right]$
]

.pull-right[
- $\mathop{E}\left[ \text{Wage}_i \mid \text{Gender}_i \right]$
]
---
layout: false
class: clear, middle

Graphically (From another of [Ed Rubin's Classes](https://github.com/edrubin/EC607S21))...
---
name: graphically

# The conditional distributions of $Y_{i}$ for $X_{i}=x$ 

```{R, data_cef, echo = F, cache = T, fig.height = 5}
# Set seed
set.seed(12345)
# Sample size
n <- 1e4
# Generate extra disturbances
u <- sample(-2:2, size = 22, replace = T) * 1e3
# Generate data
cef_df <- tibble(
  x = sample(x = seq(8, 22, 1), size = n, replace = T),
  y = 15000 + 3000 * x  + 1e3 * (x %% 3) + 500 * (x %% 2) + rnorm(n, sd = 1e4) + u[x]
) %>% mutate(x = round(x)) %>%
filter(y > 0)
# Means
means_df <- cef_df %>% group_by(x) %>% summarize(y = mean(y))
# The CEF in ggplot
gg_cef <- ggplot(data = cef_df, aes(x = y, y = x %>% as.factor())) +
  geom_density_ridges_gradient(
    aes(fill = ..x..),
    rel_min_height = 0.003,
    color = "white",
    scale = 2.5,
    size = 0.3
  ) +
  scale_x_continuous(
    "Annual income",
    labels = scales::dollar
  ) +
  ylab("Years of education") +
  scale_fill_viridis(option = "magma") +
  theme_pander(base_family = "Fira Sans Book", base_size = 18) +
  theme(
    legend.position = "none"
  ) +
  coord_flip()
```


<br>


```{R, fig_cef_dist, echo = F, cache = T, fig.height = 5}
gg_cef
```
---
# The conditional distributions of $Y_{i}$ for $X_{i}=x$ 

The CEF, $\mathop{E}\left[ \text{Y}_{i}\mid \text{X}_{i} \right]$, connects these conditional distributions' means.

```{R, fig_cef, echo = F, cache = T, fig.height = 5}
gg_cef +
  geom_path(
    data = means_df,
    aes(x = y, y = x %>% as.factor(), group = 1),
    color = "white",
    alpha = 0.85
  ) +
  geom_point(
    data = means_df,
    aes(x = y, y = x %>% as.factor()),
    color = "white",
    shape = 16,
    size = 3.5
  )
```

---
# The conditional distributions of $Y_{i}$ for $X_{i}=x$ 

Focusing in on the CEF, $\mathop{E}\left[ \text{Y}_{i}\mid \text{X}_{i} \right]$...

```{R, fig_cef_only, echo = F, cache = T, fig.height = 5}
ggplot(data = cef_df, aes(x = y, y = x %>% as.factor())) +
  geom_density_ridges(
    rel_min_height = 0.003,
    color = "grey85",
    fill = NA,
    scale = 2.5,
    size = 0.3
  ) +
  scale_x_continuous(
    "Annual income",
    labels = scales::dollar
  ) +
  ylab("Years of education") +
  scale_fill_viridis(option = "magma") +
  theme_pander(base_family = "Fira Sans Book", base_size = 18) +
  theme(
    legend.position = "none"
  ) +
  geom_path(
    data = means_df,
    aes(x = y, y = x %>% as.factor(), group = 1),
    color = "grey20",
    alpha = 0.85
  ) +
  geom_point(
    data = means_df,
    aes(x = y, y = x %>% as.factor()),
    color = "grey20",
    shape = 16,
    size = 3.5
  ) +
  coord_flip()
```
---
# The CEF is a Function of One or More Xs

- **The CEF is a function of the variables and values we are conditioning on**. For the expression: 
$$
\begin{equation}
CEF(X_i = x) = \mathop{\mathbb{E}}(Y_i|X_{1i}) =  \mathop{\mathbb{E}}(Y_i|\underbrace{X_{1i}=x}_{\text{this is what varies}}) 
\end{equation}
$$
- In a somewhat confusing convention (to me!), you will usually see that the CEF is presented as the expression in the middle above, in terms of the variable $(X_{1i})$, and not the possible values $(x)$.  

- The next key step is to realize that, the same way we can condition over one variable  $(X_{1i})$, we can condition over many other variables too: 

$$
\begin{equation}
 \mathop{\mathbb{E}}(Y_i|X_{1i} = x_1, X_{2i} = x_2,...,  X_{Ki} = x_K,) =  \mathop{\mathbb{E}}(Y_i|X_{1i}, X_{2i}, ..., X_{Ki}) 
\end{equation}
$$
---
# CEF With Multiple Xs (in English) 1/2

$$
\begin{equation}
 \mathop{\mathbb{E}}(Y_i|X_{1i} = x_1, X_{2i} = x_2,...,  X_{Ki} = x_K) =  \mathop{\mathbb{E}}(Y_i|X_{1i}, X_{2i}, ..., X_{Ki}) 
\end{equation}
$$

- In the same way $E[Y|X=13]$ can represent the answer to the question “what is the average earning for individuals with 13 years of education?”, we can also ask “what is the average earnings for individuals with 13 years of education, and residing in California?” or $E[Y|X_1=13, X_2=6]$ where 6 represents a numerical code for the state of California. 

- For this case, the CEF would be written as $E[Y|X_1, X_2]$

---
# CEF With Multiple Xs (in English) 2/2

- As the number of $X's$ grow, it becomes unwieldy notation, hence from now on we use $X$ to refer to "one or more variables", leaving us with a general expression for the CEF: 
$$
\begin{equation}
 \mathop{\mathbb{E}}(Y_i|X) 
\end{equation}
$$

- As quick trick to never forget that the CEF depends on $X$ (and not on $Y$), every time you see the expression $\mathop{\mathbb{E}}(Y_i|X)$ replace it in your mind with some function $g(X)$.


---
count:false

# Regression as Conditional Expectation: Outline

- Conditional Expectation and the Conditional Expectation Function (CEF).
  - Definitions
  - **Example**
  - Why should we care about it (regardless of regression)?
- Connection between CEF and regression.
- Use CEF to solidify our understanding of how regression **is** a simple difference in groups, for groups defined by values of regressors.  

---
# CEF: Example

- Let's bring this abstract concept down to hearth for the case of private/public colleges. 
- Before drawing the connection to regression, we can still think of the conditional expectation of our variable of interest ( $Y_i =$ earnings), and how it changes for all our other (explanatory) variables: 

$$
\begin{equation}
 \mathop{\mathbb{E}}(Y_i|X) =  \mathop{\mathbb{E}}(Y_i|P_i, GROUP_i, SAT_i, lnPI_i) 
\end{equation}
$$

- Where $GROUP_i$ represents the collection of 150 binary variables. 
- Notice the until now, we have not said anything about the functional form of the CEF, to make it more explicit, let's bring the $g()$ notation: 

$$
\begin{equation}
 \mathop{\mathbb{E}}(Y_i|P_i, GROUP_i, SAT_i, lnPI_i)  = g(P_i, GROUP_i, SAT_i, lnPI_i)
\end{equation}
$$

---
count:false

# Regression as Conditional Expectation: Outline

- Conditional Expectation and the Conditional Expectation Function (CEF).
  - Definitions
  - Example
  - **Why should we care about it (regardless of regression)?**
- Connection between CEF and regression.
- Use CEF to solidify our understanding of how regression **is** a simple difference in groups, for groups defined by values of regressors.  


---
# But Why Should we Care About the CEF? 

- The CEF is good summary of relationship between $Y$ and $X$ because:

  - (Informally) Averages are good ways of summarizing random variables (and $Y|X$ is a RV)  
  - (Formally) The CEF is the best predictor of $Y|X$ in the sense that it solves the Minimum Mean Squared Error (MMSE) problem $(\arg \min_{m(X)}\{E(Y-m(X)^2\})$. In English: given X, our best guess about $Y$, ,the one that will render the smallest mistake, will be its CEF. 
  
  - This last point sounds similar to regression, but up to this point we have not run any regressions!

---
count:false

# Regression as Conditional Expectation: Outline

- Conditional Expectation and the Conditional Expectation Function (CEF).
  - Definitions
  - Example
  - Why should we care about it (regardless of regression)?
- **Connection between CEF and regression.**
- Use CEF to solidify our understanding of how regression **is** a simple difference in groups, for groups defined by values of regressors.  

---
# Connection Between CEF and Regression 1/4
- Today we have seen that the CEF is good for summarizes relationships in the data
- Before today we have also see that Regression provides simple and powerful insights about relationships in the data

- Let’s connect the two. 

- Here we focus on the regression estimates for the population (remember we can think of regression as minimizing a population or a sample problem). This parameters correspond to: 
$$
\begin{equation}
 \{\alpha, \beta\} = \arg \min_{a,b} \left\{ 
  \mathop{\mathbb{E}}[(Y_i - a - b X_i)^2]
  \right\}
\end{equation}
$$
- Let's re-write that expression allowing for more regressors
---
# Connection Between CEF and Regression 2/4


- Now we are allowing to have more than one regressor, so let's redefine $\beta$ as the collection of all parameters that solve the minimization (what before was $\alpha, \beta, \delta, \gamma, ...$ ), and $X$ as the collection of all regressors. This allows us to write the above expression in a more compact way (representing more regressors!): 
$$
\begin{equation}
 \beta = \arg \min_{b} \left\{ 
  \mathop{\mathbb{E}}[(Y_i - X_ib)^2]
  \right\}
\end{equation}
$$

(as we increase the number of regressors the minimization requires linear algebra in addition to calculus. This is the solution of OLS with multiple variables: $\beta = \mathop{E}\left[ \text{X}_{i} \text{X}_{i}' \right]^{-1} \mathop{E}\left[ \text{X}_{i} \text{Y}_{i} \right]$ where each element represents matrices of different dimensions. This expression is the matrix version of $\beta = \frac{Cov(X_i, Y_i)}{Var(X_i)}$  and of $\alpha = \mathop{\mathbb{E}}[Y_i] - b\mathop{\mathbb{E}}[X_i]$  combined!)

---
# Connection Between CEF and Regression 3/4

- Basic claim connection CEF and Regression: if you we interested in the CEF (as summary of relationships), we should be interested in the coefficients that solve the regression (minimization) problem. 

- Let review two properties to support this claim: 

- **Property 1**: If the CEF is linear, a regression estimation (minimization) will obtain the CEF parameters (“If the CEF is linear, regression will find it”). But linearity of CEF is a strong assumption, CEF is linear when underlying data is normal, this is not very common. Using MM words: “If the CEF is linear, regression will find it”.


---
# Connection Between CEF and Regression 4/4

- **Property 2:** If the CEF is not linear, regression estimates will find the best linear approximation to the CEF in the sense that the regression coefficients will be the solution to a minimization problem with respect to $\mathop{\mathbb{E}}[Y_i|X]$, which is different from $Y_i$ $(\arg \min_{b}(E( \mathop{\mathbb{E}}[Y_i|X] - X’b )^2)$. Using MM words: "If the CEF is not linear, regression finds a good approximation to it". 

- (if you are interested in the proofs for these check [this slides](https://raw.githack.com/edrubin/EC607S21/master/notes-lecture/03-why-regression/03-why-regression.html#38), it all starts from Adam's Law!)

- Let's go back to our example to see this properties in practice. 



---
# Connection Between CEF and Regression 5/4

- We define the CEF for the private/public example as follows: 

$$
\begin{equation}
 \mathop{\mathbb{E}}(Y_i|P_i, GROUP_i, SAT_i, lnPI_i)  = g(P_i, GROUP_i, SAT_i, lnPI_i)
\end{equation}
$$
- Now, to explore property 1, we will **assume** that the CEF of $lnY_i$ is linear in X's: 
$$
\begin{equation}
 \mathop{\mathbb{E}}(ln Y_i|X) =  \theta_1 + \theta_2 P_i +\sum_{j=1}^{150} \theta_{3,j} GROUP_{ji} + \theta_4 SAT + \theta_4 ln PI_{i}
\end{equation}
$$
- What property 1 tells us is that in this case, the OLS regression for $\beta$ (define as all the collection of parameters) will produce as result the same parameters of the CEF above (i.e, $\beta = \theta$ ).



---
class: clear, middle

.hi-slate[Q] How does the CEF relate to/inform regression?
---
layout: true
# Regression
## The *CEF*
---
name: lie

As we derive the properties and relationships associated with the CEF, regression, and a host of other estimators, we will frequently rely upon<br>.hi-slate[*the Law of Iterated Expectations*] (LIE).
--
$$
\begin{align}
  \color{#6A5ACD}{\mathop{E}\left[ \text{Y}_{i} \right]} = \mathop{E}\!\bigg( \color{#e64173}{\mathop{E}\left[ \text{Y}_{i}\mid \text{X}_{i} \right]} \bigg)
\end{align}
$$
--
which says that the .hi-purple[unconditional expectation] is equal to the .b[unconditional average] of the .hi-pink[conditional expectation function].

---
layout: false
class: clear, middle

Great. What's the point?
---
name: decomposition
.font90[
#.hi-slate[Theorem] The CEF decomposition property (3.1.1)

The LIE allows us to **decompose random variables** into two pieces

$$
\begin{align}
  \text{Y}_{i} = \color{#e64173}{\mathop{E}\left[ \text{Y}_{i}\mid \text{X}_{i} \right]} + \color{#6A5ACD}{\varepsilon_i}
\end{align}
$$

1. .hi-pink[the conditional expectation function]
2. .hi-purple[a residual] with special powers
<br> i.  $\color{#6A5ACD}{\varepsilon_i}$ is mean independent of $\text{X}_{i}$, _i.e._, $\mathop{E}\left[ \color{#6A5ACD}{\varepsilon_i} \mid \text{X}_{i} \right] = 0$.
<br> ii.  $\color{#6A5ACD}{\varepsilon_i}$ is uncorrelated with any function of $\text{X}_{i}$.


.hi-orange[*Important*] It might not seem like much, but these results are .hi-orange[huge] for building intuition, theory, *and* application.

 Put a ⭐ here!
 ]

---

#The CEF decomposition property

<br>says that we can decompose any random variable (_e.g._, $\text{Y}_{i}$) into

1. a part that is .pink[explained by] $\color{#e64173}{\text{X}_{i}}$ (_i.e._, the CEF $\color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]}$),
2. a part that is .purple[*orthogonal to*<sup>.purple[†]</sup> any function of] $\color{#6A5ACD}{\text{X}_{i}}$ (_i.e._, $\color{#6A5ACD}{\varepsilon_i}$).

.footnote[.purple[†] "orthogonal to" = "uncorrelated with"]

--

.hi-slate[Why the CEF?]
<br>The .pink[CEF] also presents an intuitive summary of the relationship between $\text{Y}_{i}$ and $\text{X}_{i}$, since we are often use means to characterize random variables.

--

But (of course) there are more reasons to use the CEF...
---
name: prediction

# Theorem: The CEF prediction property 

Let $\mathop{m}\left( \text{X}_{i} \right)$ be *any* function of $\text{X}_{i}$. The CEF solves
$$
\begin{align}
  \color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]} = \underset{\mathop{m}\left( \text{X}_{i} \right)}{\text{arg min}}\enspace \mathop{E}\left[ \left( \text{Y}_{i} - \mathop{m}\left( \text{X}_{i} \right) \right)^2 \right]
\end{align}
$$
In other words, the .hi-pink[CEF] is the minimum mean-squared error (MMSE) predictor of $\text{Y}_{i}$ given $\text{X}_{i}$.

--

.hi-slate[*Notice*]
1. We haven't restricted $m$ to any class of functions—it can be nonlinear.
2. We're talking about *prediction* (specifically predicting $\text{Y}_{i}$).

---
# ANOVA Theorem

One final property of the .pink[CEF] (very similar to the decomposition property)

.hi-slate[Theorem] The ANOVA theorem (3.1.3)
$$
\begin{align}
  \mathop{\text{Var}} \left( \text{Y}_{i} \right) = \mathop{\text{Var}} \left( \color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]} \right) + \mathop{E}\left[ \mathop{\text{Var}} \left( \text{Y}_{i} \mid \text{X}_{i} \right) \right]
\end{align}
$$
which says that we can decompose the variance in $\text{Y}_{i}$ into
1. the variance in the .pink[CEF]
2. the variance of the residual

.hi-slate[*Example*] Decomposing wage variation into (.hi-slate[1]) variation explained by workers' characteristics and (.hi-slate[2]) unexplained (residual) variation

---
layout: false
class: clear, middle

We now understand the CEF a bit better.
<br>But how does the CEF actually relate to regression?
---
layout: true
# Regression
## The *CEF* and regression
---

We've discussed how the .pink[CEF] summarizes empirical relationships.

*Previously* we discussed how regression provides simple empirical insights.

Let's link these two concepts.
---
name: pop_ls

.hi-slate[Population least-squares regression]

We will focus on $\beta$, the vector (a $K\times 1$ matrix) of population, least-squares regression coefficients, _i.e._,
$$
\begin{align}
  \beta = \underset{b}{\text{arg min}}\thinspace \mathop{E}\left[ \left( \text{Y}_{i} - \text{X}_{i}'b \right)^2 \right]
\end{align}
$$
where $b$ and $\text{X}_{i}$ are also $K\times 1$, and $\text{Y}_{i}$ is a scalar.

--

Taking the first-order condition gives
$$
\begin{align}
   \mathop{E}\left[ \text{X}_{i} \left( \text{Y}_{i} - \text{X}_{i}'b \right) \right] = 0
\end{align}
$$
---

From the first-order condition
$$
\begin{align}
   \mathop{E}\left[ \text{X}_{i} \left( \text{Y}_{i} - \text{X}_{i}'b \right) \right] = 0
\end{align}
$$
we can solve for $b$. We've defined the optimum as $\beta$. Thus,
$$
\begin{align}
  \beta = \mathop{E}\left[ \text{X}_{i} \text{X}_{i}' \right]^{-1} \mathop{E}\left[ \text{X}_{i} \text{Y}_{i} \right]
\end{align}
$$

--

.hi-slate[*Note*] The first-order conditions tell us that our least-squares population regression residuals $\left(e_i = \text{Y}_{i} - \text{X}_{i}'\beta \right)$ are uncorrelated with $\text{X}_i$.
---
layout: true

# Regression
## Anatomy
---
name: anatomy

Our "new" result: $\beta = \mathop{E}\left[ \text{X}_{i} \text{X}_{i}' \right]^{-1} \mathop{E}\left[ \text{X}_{i} \text{Y}_{i} \right]$

In .hi-slate[simple linear regression] (an intercept and one regressor $x_i$),
$$
\begin{align}
  \beta_1 &= \dfrac{\mathop{\text{Cov}} \left( \text{Y}_{i},\, x_i \right)}{\mathop{\text{Var}} \left( x_i \right)}
  & \beta_0 = \mathop{E}\left[ \text{Y}_{i} \right] - \beta_1 \mathop{E}\left[ x_i \right]
\end{align}
$$

--

For .hi-slate[multivariate regression], the coefficient on the k.super[th] regressor $x_{ki}$ is
$$
\begin{align}
  \beta_k &= \dfrac{\mathop{\text{Cov}} \left( \text{Y}_{i},\, \widetilde{x}_{ki} \right)}{\mathop{\text{Var}} \left( \widetilde{x}_{ki} \right)}
\end{align}
$$
where $\widetilde{x}_{ki}$ is the residual from a regression of $x_{ki}$ on all other covariates.
---

This alternative formulation of least-squares coefficients is quite powerful.
$$
\begin{align}
  \beta_k &= \dfrac{\mathop{\text{Cov}} \left( \text{Y}_{i},\, \widetilde{x}_{ki} \right)}{\mathop{\text{Var}} \left( \widetilde{x}_{ki} \right)}
\end{align}
$$

--

.hi-slate[Why?]
--
 This expression illustrates how each coefficient in a least-squares regression represents the bivariate slope coefficient .pink[after controlling for the other covariates].

---

In fact, we can re-write our coefficients to further emphasize this point
$$
\begin{align}
  \beta_k &= \dfrac{\mathop{\text{Cov}} \left( \widetilde{\text{Y}}_{i},\, \widetilde{x}_{ki} \right)}{\mathop{\text{Var}} \left( \widetilde{x}_{ki} \right)}
\end{align}
$$
$\widetilde{\text{Y}}_{i}$ denotes the residual from regressing $\text{Y}_{i}$ on all regressors except $x_{ki}$.
---
layout: false
class: clear, middle

Graphical example
---
```{R, data_anatomy, cache = F, include = F, fig.height = 4 }
n <- 1e2
set.seed(1234)
gen_df <- tibble(
  x2 = sample(x = c(F, T), size = n, replace = T),
  x1 = runif(n = n, min = -3, max = 3) + x2 - 0.5,
  u  = rnorm(n = n, mean = 0, sd = 1),
  y  = -3 + x1 + x2 * 4 + u
)
ya_mean <- filter(gen_df, x2 == F)$y %>% mean()
yb_mean <- filter(gen_df, x2 == T)$y %>% mean()
x1a_mean <- filter(gen_df, x2 == F)$x1 %>% mean()
x1b_mean <- filter(gen_df, x2 == T)$x1 %>% mean()
gen_df %<>% mutate(
  y_dm = y - ya_mean * (x2 == F) - yb_mean * (x2 == T),
  x1_dm = x1 - x1a_mean * (x2 == F) - x1b_mean * (x2 == T)
)
```

$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \varepsilon_i$

```{R, fig_anatomy1, echo = F, fig.height = 4}
ggplot(data = gen_df, aes(y = y, x = x1, color = x2, shape = x2)) +
  geom_hline(yintercept = 0, color = "grey85") +
  geom_vline(xintercept = 0, color = "grey85") +
  geom_point(size = 3) +
  annotate("text", x = -0.075, y = 6.75, label = TeX("$y$"), size = 8) +
  annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1$"), size = 8) +
  ylim(c(-7, 7)) +
  xlim(c(-3.5, 3.5)) +
  theme_empty +
  scale_color_manual(
    expression(x[2]),
    values = c("darkslategrey", red_pink),
    labels = c("A", "B")
  ) +
  scale_shape_manual(
    expression(x[2]),
    values = c(1, 19),
    labels = c("A", "B")
  ) +
  theme(
    legend.position = "bottom",
    text = element_text(size = 22)
  )
```
---
class: clear, middle, center
count: false

$\beta_1$ gives the relationship between $y$ and $x_1$ *after controlling for* $x_2$

```{R, fig_anatomy2, echo = F, fig.height = 4}
ggplot(data = gen_df, aes(y = y, x = x1, color = x2, shape = x2)) +
  geom_hline(yintercept = ya_mean, color = "darkslategrey", alpha = 0.5, size = 2) +
  geom_hline(yintercept = yb_mean, color = red_pink, alpha = 0.5, size = 2) +
  geom_hline(yintercept = 0, color = "grey85") +
  geom_vline(xintercept = 0, color = "grey85") +
  geom_point(size = 3) +
  annotate("text", x = -0.075, y = 6.75, label = TeX("$y$"), size = 8) +
  annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1$"), size = 8) +
  ylim(c(-7, 7)) +
  xlim(c(-3.5, 3.5)) +
  theme_empty +
  scale_color_manual(
    expression(x[2]),
    values = c("darkslategrey", red_pink),
    labels = c("A", "B")
  ) +
  scale_shape_manual(
    expression(x[2]),
    values = c(1, 19),
    labels = c("A", "B")
  ) +
  theme(
    legend.position = "bottom",
    text = element_text(size = 22)
  )
```
---
class: clear, middle, center
count: false

$\beta_1$ gives the relationship between $y$ and $x_1$ *after controlling for* $x_2$

```{R, fig_anatomy3, echo = F, fig.height = 4}
ggplot(data = gen_df %>% mutate(y = y - 4 * x2), aes(y = y_dm, x = x1)) +
  geom_hline(yintercept = 0, color = "grey85") +
  geom_vline(xintercept = 0, color = "grey85") +
  geom_point(size = 3, aes(color = x2, shape = x2)) +
  annotate("text", x = -0.075, y = 6.75, label = TeX("$y|x_2$"), size = 8, hjust = 1) +
  annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1$"), size = 8) +
  ylim(c(-7, 7)) +
  xlim(c(-3.5, 3.5)) +
  theme_empty +
  scale_color_manual(
    expression(x[2]),
    values = c("darkslategrey", red_pink),
    labels = c("A", "B")
  ) +
  scale_shape_manual(
    expression(x[2]),
    values = c(1, 19),
    labels = c("A", "B")
  ) +
  theme(
    legend.position = "bottom",
    text = element_text(size = 22)
  )
```
---
class: clear, middle, center
count: false

$\beta_1$ gives the relationship between $y$ and $x_1$ *after controlling for* $x_2$

```{R, fig_anatomy4, echo = F, fig.height = 4}
ggplot(data = gen_df %>% mutate(y = y - 4 * x2), aes(y = y_dm, x = x1)) +
  geom_hline(yintercept = 0, color = "grey85") +
  geom_vline(xintercept = 0, color = "grey85") +
  geom_vline(xintercept = x1a_mean, color = "darkslategrey", alpha = 0.5, size = 2) +
  geom_vline(xintercept = x1b_mean, color = red_pink, alpha = 0.5, size = 2) +
  geom_point(size = 3, aes(color = x2, shape = x2)) +
  annotate("text", x = -0.075, y = 6.75, label = TeX("$y|x_2$"), size = 8, hjust = 1) +
  annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1$"), size = 8) +
  ylim(c(-7, 7)) +
  xlim(c(-3.5, 3.5)) +
  theme_empty +
  scale_color_manual(
    expression(x[2]),
    values = c("darkslategrey", red_pink),
    labels = c("A", "B")
  ) +
  scale_shape_manual(
    expression(x[2]),
    values = c(1, 19),
    labels = c("A", "B")
  ) +
  theme(
    legend.position = "bottom",
    text = element_text(size = 22)
  )
```
---
class: clear, middle, center
count: false

$\beta_1$ gives the relationship between $y$ and $x_1$ *after controlling for* $x_2$

```{R, fig_anatomy5, echo = F, fig.height = 4}
ggplot(data = gen_df %>% mutate(y = y - 4 * x2), aes(y = y_dm, x = x1_dm)) +
  geom_hline(yintercept = 0, color = "grey85") +
  geom_vline(xintercept = 0, color = "grey85") +
  geom_point(size = 3, aes(color = x2, shape = x2)) +
  annotate("text", x = -0.075, y = 6.75, label = TeX("$y|x_2$"), size = 8, hjust = 1) +
  annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1|x_2$"), size = 8, hjust = 1) +
  ylim(c(-7, 7)) +
  xlim(c(-3.5, 3.5)) +
  theme_empty +
  scale_color_manual(
    expression(x[2]),
    values = c("darkslategrey", red_pink),
    labels = c("A", "B")
  ) +
  scale_shape_manual(
    expression(x[2]),
    values = c(1, 19),
    labels = c("A", "B")
  ) +
  theme(
    legend.position = "bottom",
    text = element_text(size = 22)
  )
```
---
class: clear, middle, center
count: false

$\beta_1$ gives the relationship between $y$ and $x_1$ *after controlling for* $x_2$

```{R, fig_anatomy6, echo = F, fig.height = 4}
ggplot(data = gen_df %>% mutate(y = y - 4 * x2), aes(y = y_dm, x = x1_dm)) +
  geom_hline(yintercept = 0, color = "grey85") +
  geom_vline(xintercept = 0, color = "grey85") +
  geom_smooth(method = lm, se = F, color = purple, alpha = 1, size = 2) +
  geom_point(size = 3, aes(color = x2, shape = x2)) +
  annotate("text", x = -0.075, y = 6.75, label = TeX("$y|x_2$"), size = 8, hjust = 1) +
  annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1|x_2$"), size = 8, hjust = 1) +
  ylim(c(-7, 7)) +
  xlim(c(-3.5, 3.5)) +
  theme_empty +
  scale_color_manual(
    expression(x[2]),
    values = c("darkslategrey", red_pink),
    labels = c("A", "B")
  ) +
  scale_shape_manual(
    expression(x[2]),
    values = c(1, 19),
    labels = c("A", "B")
  ) +
  theme(
    legend.position = "bottom",
    text = element_text(size = 22)
  )
```


---
layout: false
class: clear, middle

Now that we've refreshed/deepened our regression knowledge, let's connect regression and the CEF.
---
layout: true
# Regression
## Regression and the *CEF*
---

Angrist and Pischke make the case that
> ... you should be interested in regression parameters if you are interested in the CEF. (*MHE*, p.36)

.hi-slate[Q] What is the reasoning/connection?

--

.hi-slate[A] We'll cover three reasons.

1. *If the CEF is linear*, then the population regression line is the CEF.

2. The function $\text{X}_{i}' \beta$ is the min. MSE *linear* predictor of $\text{Y}_{i}$ given $\text{X}_{i}$.

3. The function $\text{X}_{i}' \beta$ gives the min. MSE *linear* approximation to the CEF.
---
layout: true
# Regression
## Regression and the *CEF*
---

.hi-slate[Theorem] The linear CEF theorem (3.1.4)

If the CEF is linear, then the population regression is the CEF.

--

.hi-slate[Proof] Let the CEF equal some linear function, _i.e._, $\color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]} = \text{X}_{i}' \beta^\star$.

From the CEF decomposition property, we know $\mathop{E}\left[ \text{X}_{i} \color{#6A5ACD}{\varepsilon_i} \right] = 0$.

--

$\implies \mathop{E}\left[ \text{X}_{i} \left( \text{Y}_{i} - \color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]} \right) \right] = 0$

--

$\implies \mathop{E}\left[ \text{X}_{i} \left( \text{Y}_{i} - \text{X}_{i}'\beta^\star \right) \right] = 0$

--

$\implies \mathop{E}\left[ \text{X}_{i}\text{Y}_{i} \right] - \mathop{E}\left[ \text{X}_{i} \text{X}_{i}' \beta^\star \right] = 0$

--

$\implies \beta^\star = \mathop{E}\left[ \text{X}_{i} \text{X}_{i}' \right]^{-1} \mathop{E}\left[ \text{X}_{i}\text{Y}_{i} \right]$
--
 $=\beta$, our population regression coefficients.
---

>.hi-slate[Theorem] The linear CEF theorem (3.1.4)
>
>If the CEF is linear, then the population regression is the CEF.

Linearity can be a strong assumption. When might we expect linearity?

--

1. Situations in which $\left( \text{Y}_{i},\, \text{X}_{i} \right)$ follows a multivariate normal distribution.
<br>.hi-slate[*Concern*] Might be limited—especially when $\text{Y}_{i}$ or $\text{X}_{i}$ are not continuous.

--

2. Saturated regression models
<br>.hi-slate[*Example*] A model with two binary indicators and their interaction.

---

.hi-slate[Theorem] The best linear predictor theorem (3.1.5)

$\text{X}_{i}' \beta$ is the best *linear* predictor of $\text{Y}_{i}$ given $\text{X}_{i}$ (minimizes MSE).

--

.hi-slate[Proof] We defined $\beta$ as the vector that minimizes MSE, _i.e._,
$$
\begin{align}
  \beta = \underset{b}{\text{arg min}}\thinspace \mathop{E}\left[ \left( \text{Y}_{i} - \text{X}_{i}'b \right)^2 \right]
\end{align}
$$
so $\text{X}_{i}'\beta$ is literally defined as the minimum MSE linear predictor of $\text{Y}_{i}$.

--

- The population-regression function $\left(\text{X}_{i}'\beta\right)$ is the best (min. MSE) *linear* predictor of $\text{Y}_{i}$ given $\text{X}_{i}$.
- The CEF $\left( \mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right] \right)$ is the best predictor (min. MSE) of $\text{Y}_{i}$ given $\text{X}_{i}$ across *all classes* of functions.

---

.hi-slate[Q] If $\text{X}_{i}'\beta$ is .hi[the best linear predictor] of $\text{Y}_{i}$ given $\text{X}_{i}$, then why is there so much interest machine learning for prediction (opposed to regression)?

--

.hi-slate[A] A few reasons

1. Relax *linearity*
2. Model selection
  - choosing $\text{X}_{i}$ is not always obvious
  - overfitting is bad (bias-variance tradeoff)
3. It's fancy, shiny, and new
4. Some ML methods boil down to regression
5. Others?

--

.hi-slate[Counter Q] Why are we (still) using regression?
---
name: reg_cef_theorem

.hi-slate[Theorem] The regression CEF theorem (3.1.6)

The population regression function $\text{X}_{i}'\beta$ provides the minimum MSE linear approximation to the CEF $\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]$, _i.e._,
$$
\begin{align}
  \beta = \underset{b}{\text{arg min}}\thinspace \mathop{E}\!\left\{ \bigg( \mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right] - \text{X}_{i}'b \bigg)^2 \right\}
\end{align}
$$

--

.hi-slate[*Put simply*] Regression gives us the *best* linear approximation to the CEF.
---
layout: false
class: clear

.hi-slate[Proof] First, recall that, .orange[in expectation], $\beta$ is the $b$ that minimizes $\left( \text{Y}_{i} - \text{X}_{i}'b \right)^2$

--

$\left( \text{Y}_{i} - \text{X}_{i}'b \right)^2 \color{#ffffff}{\bigg|}$ .right10[.orange[(**1**)]]
--
<br>  $= \bigg( \left\{ \text{Y}_{i} - \color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]} \right\} + \left\{ \color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]} - \text{X}_{i}'b \right\} \bigg)^2$
--
<br>  $= \bigg( \text{Y}_{i} - \color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]} \bigg)^2$ .right10[.turquoise[(**a**)]]
<br>  $\color{#ffffff}{=}+\bigg( \color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]} - \text{X}_{i}'b \bigg)^2$ .right10[.turquoise[(**b**)]]
<br>  $\color{#ffffff}{=}+ 2 \bigg( \text{Y}_{i} - \color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]} \bigg) \bigg( \color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]} - \text{X}_{i}'b \bigg)$ .right10[.turquoise[(**c**)]]

--

We want to minimize .turquoise[(**b**)], and we know $\beta$ minimizes .orange[(**1**)].
--
<br> .turquoise[(**a**)] is irrelevant, _i.e._,  it does not depend upon $b$.
--
<br> .turquoise[(**c**)] can be written as $2 \color{#6A5ACD}{\varepsilon_i} \mathop{h}(\text{X}_{i})$, which equals zero in expectation.

--

∴ (In expectation) If $b=\beta$ minimizes .orange[(**1**)], then $b=\beta$ minimizes .turquoise[(**b**)].

---
layout: true
# Regression
## Regression and the *CEF*
---

Let's review our new(-ish) regression results

1. When the CEF is linear, the regression function *is* the CEF.
<br>.hi-slate[Too small] Very specific circumstances—or big assumptions.

1. Regression gives us the best *linear* predictor of $\text{Y}_{i}$ (given $\text{X}_{i}$)
<br>.hi-slate[Off point] We're often interested in $\beta$—not $\widehat{\text{Y}}_{i}$.

1. Regression provides the best *linear* approximation of the CEF.
<br>.hi-slate[Just right?] (Depends on your goals)

---

Motivation (**3**) tends to be the most compelling.

Even when the CEF is not linear, regression recovers the best linear approximation to the CEF.

--

> The statement that .pink[regression approximates the CEF] lines up with our view of .purple[empirical work as an effort to describe the essential features of statistical relationships] without necessarily trying to pin them down exactly. (*MHE*, p.39, emphasis added)
---
layout: false
class: clear, middle

Let's dig into this linear-approximate to the CEF a little more...
---
class: clear, middle, center

Returning to our **CEF**

```{R, fig_reg_cef, echo = F, cache = T}
# Estimate the relationship
cef_lm <- lm(y ~ x, data = cef_df)
# Find the regression points
lm_df <- tibble(
  x = 8:22,
  y = predict(object = cef_lm, newdata = data.frame(x = 8:22))
)
# Create the figure
gg_cef <- ggplot(data = cef_df, aes(x = y, y = x %>% as.factor())) +
  geom_density_ridges(
    rel_min_height = 0.003,
    color = "grey85",
    fill = NA,
    scale = 2.5,
    size = 0.3
  ) +
  scale_x_continuous(
    "Annual income",
    labels = scales::dollar
  ) +
  ylab("Years of education") +
  scale_fill_viridis(option = "magma") +
  theme_pander(base_family = "Fira Sans Book", base_size = 18) +
  theme(
    legend.position = "none"
  ) +
  geom_path(
    data = means_df,
    aes(x = y, y = x %>% as.factor(), group = 1),
    color = "grey20",
    alpha = 0.85
  ) +
  geom_point(
    data = means_df,
    aes(x = y, y = x %>% as.factor()),
    color = "grey20",
    shape = 16,
    size = 3.5
  ) +
  coord_flip()
# Plot it
gg_cef
```
---
class: clear, center, middle

Adding the population .hi-orange[regression function]

```{R, fig_reg_cef2, echo = F, cache = T}
# Figure
gg_cef +
  geom_path(
    data = lm_df,
    aes(x = y, y = x %>% as.factor(), group = 1),
    color = orange,
    alpha = 0.5,
    size = 2
  )
```
---
layout: true

# Regression
## Regression and the *CEF*
---
name: wls

As the previous figure suggests, one way to think about least-squares regression is .hi-slate[estimating a weighted regression on the CEF] rather than the individual observations.

--

.slate[.mono[TLDR]] Use $\color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]}$ as the outcome, rather than $\text{Y}_{i}$, and properly weight.

--

Suppose $\text{X}_{i}$ is discrete with pmf $\mathop{g_x}(u)$
$$
\begin{align}
  \mathop{E}\!\bigg[ \left( \color{#e64173}{\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} \right]} - \text{X}_{i}'b \right)^2 \bigg] = \sum_u \left( \mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} = u \right] - u'b \right)^2 \mathop{g_x}(u)
\end{align}
$$
_i.e._, $\beta$ can be expressed as weighted-least squares regression of $\mathop{E}\left[ \text{Y}_{i} \mid \text{X}_{i} = u\right]$ on $u$ (the values of $\text{X}_{i}$) weighted by $\mathop{g_x}(u)$.
---

We can also use LIE here

$\beta$
--
<br>.pad-left[] $= \mathop{E}\left[ X_i X_i' \right]^{-1} \mathop{E}\left[ \text{X}_{i}\text{Y}_{i} \right]$
--
<br>.pad-left[] $= \mathop{E}\left[ X_i X_i' \right]^{-1} \mathop{E}\left[ \text{X}_{i} \mathop{E}\left( \text{Y}_{i}\mid \text{X}_{i} \right) \right]$

--

.hi-pink[Pro] Useful for aggregated data when microdata are sensitive/big.

--

.hi-pink[Con] You .hi-slate[will not] get the same standard errors.
---
layout: false
# Table of contents

.pull-left[
### Admin
.smaller[

1. [Schedule](#schedule)
]

]

.pull-right[
### Regression
.smaller[

1. [Why?](#why)
1. [The CEF](#cef)
  - [Definition](#cef)
  - [Graphically](#graphically)
  - [Law of iterated expectations](#lie)
  - [Decomposition](#decomposition)
  - [Prediction](#prediction)
1. [Population least squares](#pop_ls)
1. [Anatomy](#anatomy)
1. [Regression-CEF theorem](#reg_cef_theorem)
1. [WLS](#wls)
]
]
---
exclude: true


```{r gen_pdf, include = FALSE, cache = FALSE, eval = FALSE}
pagedown::chrome_print("13_reg_line_fit_ce.html", output = "13_reg_line_fit_ce.pdf")
```